<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Constrained Optimization 3DGS ICCV 2025</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds</span>
		<table align=center width=600px>
			<table align=center width=1200px>
				<br>
				<tr>
					<td align=center width=160px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.com/citations?user=s6JKJSymFFsC&hl=en">Jizong Peng</a><sup>1*</sup></span>
						</center>
					</td>
					<td align=center width=160px>
						<center>
							<span style="font-size:24px"><a href="https://eldentse.github.io/">Tze Ho Elden Tse</a><sup>2*</sup></span>
						</center>
					</td>
					<td align=center width=160px>
						<center>
							<span style="font-size:24px"><a href="https://kai422.github.io/">Kai Xu</a><sup>2</sup></span>
						</center>
					</td>
					<td align=center width=160px>
						<center>
							<span style="font-size:24px"><a href="https://ieeexplore.ieee.org/author/38265568800">Wenchao Gao</a><sup>1</sup></span>
						</center>
					</td>
					<td align=center width=160px>
						<center>
							<span style="font-size:24px"><a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a><sup>2</sup></span>
						</center>
					</td>
				</tr>
			</table>
			<!-- <br> -->
			<!-- <table align=center width=1200px>
				<br>
				<tr>
					<td align=center width=160px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.com/citations?user=DB8aKRgAAAAJ&hl=en">Mingsong Dou</a></span>
						</center>
					</td>
					<td align=center width=160px>
						<center>
							<span style="font-size:24px"><a href="https://www.zhangyinda.com/">Yinda Zhang</a></span>
						</center>
					</td>
					<td align=center width=160px>
						<center>
							<span style="font-size:24px"><a href="https://eldentse.github.io/">Sasa Petrovic</a></span>
						</center>
					</td>
					<td align=center width=160px>
						<center>
							<span style="font-size:24px"><a href="https://hyungjinchang.wordpress.com/">Hyung Jin Chang</a></span>
						</center>
					</td>
					<td align=center width=160px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.ca/citations?user=EEre0EcAAAAJ&hl=en">Jonathan Taylor</a></span>
						</center>
					</td>
					<td align=center width=160px>
						<center>
							<span style="font-size:24px"><a href="https://bardiadoosti.github.io/">Bardia Doosti</a></span>
						</center>
					</td>
				</tr>
			</table> -->
			<br>
			<center>
				<span style="font-size:20px">( *Equal contribution )
					</span>
			</center>
			<center>
				<span style="font-size:20px"><sup>1</sup>dConstruct Robotics, <sup>2</sup>National University of Singapore
					</span>
			</center>

			<center>
				<span style="font-size:20px">ICCV 2025
					</span>
			</center>
				
			</table>
			<br>
			<table align=center width=260px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/pdf/2504.09129'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/eldentse/contrained-optimization-3dgs/tree/main'>[Code]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<br>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:800px" src="./resources/overview-new-bigger-text.jpg"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					Given noisy point clouds and inaccurate camera poses, our constrained optimization approach reconstructs the 3D scene in Gaussian Splatting with high visual quality.
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				3D Gaussian Splatting (3DGS) is a powerful reconstruction technique, but it needs to be initialized from accurate camera poses and high-fidelity point clouds. Typically, the initialization is taken from Structure-from-Motion (SfM) algorithms; however, SfM is time-consuming and restricts the application of 3DGS in real-world scenarios and large-scale scene reconstruction. We introduce a constrained optimization method for simultaneous camera pose estimation and 3D reconstruction that does not require SfM support. Core to our approach is decomposing a camera pose into a sequence of camera-to-(device-)center and (device-)center-to-world optimizations. To facilitate, we propose two optimization 
constraints conditioned to the sensitivity of each parameter group and restricts each parameter's search space. In addition, as we learn the scene geometry directly from the noisy point clouds, we propose geometric constraints to improve the reconstruction quality. Experiments demonstrate that the proposed method significantly outperforms the existing (multi-modal) 3DGS baseline and methods supplemented by COLMAP on both our collected dataset and two public benchmarks.
			</td>
		</tr>
	</table>
	<br>

<!-- 	<hr>
	<center><h1>Talk</h1></center>
	<p align="center">
		<iframe width="660" height="395" src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
	</p> -->

<!-- 	<table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:28px"><a href=''>[Slides]</a>
				</span>
			</center>
		</tr>
	</table> -->
	<hr>

	<!-- <center><h1>Framework</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:950px" src="./resources/framework.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td align=center width=400px>
					A schematic illustration of our framework. Given multi-view RGB images, we extract volumetric features with a shared CNN backbone. 
					The soft-attention fusion block generates the attention mask and finer image features through multiple upsampling and convolution blocks. 
					Region-specific features are computed by first aggregating along the feature channel dimension via the attention mask, followed by a max-pooling operation across multi-view images to focus on
					useful features. Then, we apply mesh segmentation via spectral clustering on template hand meshes and uniformly subsample
					them to obtain coarse meshes. We perform position encoding by concatenating coarse template meshes to the corresponding
					region-specific features, i.e. matching colored features to mesh segments. Finally, our multi-layer transformer encoder
					takes the resulting features as input and outputs a coarse mesh representation which is then decoded by a spectral graph
					decoder to produce the final two-hand meshes at target resolution. Here, each hand contains 4023 vertices.
				</td>
			</tr>
		</center>
	</table>


	<hr> -->

	<center><h1>Qualitative examples our multi-camera SLAM system</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/overall-context.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td align=center width=550px style="white-space: nowrap;">
					Qualitative example of camera poses and colored point clouds obtained from our multi-camera SLAM system.
				</td>
			</tr>
		</center>
	</table>

	<hr>

	<center><h1>Illustration of camera intrinsic optimization</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:700px" src="./resources/intrinsic-explain.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td align=center width=400px>
					Illustration of camera intrinsic optimization. (a) In monocular setting, inaccurate intrinsic parameters could be corrected by adjusting the camera pose, eg. shifting the camera origin right by T. (b) This approach is not feasible for multi-cameras under extrinsic constraints like autonomous cars or SLAM devices.
				</td>
			</tr>
		</center>
	</table>

	<hr>

	<center><h1>Illustration of our camera decomposition scheme</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/pose-refinement.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td align=center width=550px>
					Illustration of our camera decomposition scheme. (a) Initial noisy point cloud from SLAM setup. (b) and (d) Optimization procedures of device-to-world and camera-to-device transformations. (c) Refined point cloud from our constrained optimization approach, showing improved visual quality.
				</td>
			</tr>
		</center>
	</table>

	<hr>

	<center><h1>Illustration of the log-barrier method</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/log-barrier.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td align=center width=550px>
					Illustration of the log-barrier method. Lower and upper bounds are predefined based on initial SLAM estimation. At the start of the optimization, the barrier imposes a strong penalty for significant deviations from the initial estimate. As temperature increases, it transforms into a well-function, allowing the parameter to fully explore the feasible region.
				</td>
			</tr>
		</center>
	</table>

<!-- 	<table align=center width=800px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;<a href='https://github.com/eldentse/collab-hand-object/'>[GitHub]</a>
			</center>
		</span>
	</table> -->

	<hr>
	<center><h1>Video</h1></center>
	<p align="center">
		<iframe width="660" height="395" src="https://www.youtube.com/embed/cfsk5e5C_Xs?si=iQ9PUM3J7WiMtDA1" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
	</p> 

	<br>
	<hr>
	<table align=center width=800px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">Jizong Peng, Tze Ho Elden Tse, Kai Xu, Wenchao Gao and Angela Yao<br>
				<b>A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds</b><br>
				In ICCV, 2025.<br>
<!-- 				(hosted on <a href="">ArXiv</a>)<br> -->
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
<!-- 			<td align=center><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td> -->
				
			<td align=center><span style="font-size:14pt"><center>
				<a href="https://arxiv.org/pdf/2504.09129">[Paper]</a>
			</center></td>
			
			<td align=center><span style="font-size:14pt"><center>
				<a href="https://arxiv.org/pdf/2504.09129">[Supplementary]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<!-- <table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This research was supported by the MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information Technology Research Center) 
					support program (IITP-2023-2020-0-01789), supervised by the IITP (Institute for Information & Communications Technology Planning & Evaluation).
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table> -->

<br>
</body>
</html>

